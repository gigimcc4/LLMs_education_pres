<!DOCTYPE html>
<html lang="en"><head>
<script src="Index_files/libs/clipboard/clipboard.min.js"></script>
<script src="Index_files/libs/quarto-html/tabby.min.js"></script>
<script src="Index_files/libs/quarto-html/popper.min.js"></script>
<script src="Index_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="Index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Index_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="Index_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="Index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.353">

  <title>Innovations in Educational Research Using Large Language Models (LLMs)</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="Index_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="Index_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="Index_files/libs/revealjs/dist/theme/quarto.css">
  <link href="Index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="Index_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="Index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="Index_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="img/title_2.jpg" data-background-size="cover" class="quarto-title-block center">
  <h1 class="title">Innovations in Educational Research Using Large Language Models (LLMs)</h1>
  <p class="subtitle"><em>Enhancing Qualitative Analysis and Addressing Data Imbalances</em></p>

<div class="quarto-title-authors">
</div>

</section>
<section id="challenges-in-education-research" class="slide level2" data-background-image="img/background.jpg">
<h2>Challenges in Education Research</h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><br></p>
<div class="fragment">
<!-- -->
<h3 id="qualitative"><strong>Qualitative</strong></h3>
<ul>
<li class="fragment">Qualitative Coding and Analysis</li>
<li class="fragment">Data Saturation and Analysis Depth</li>
</ul>
</div>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:50%;">
<p><br></p>
<div class="fragment">
<!-- -->
<h3 id="quantitative"><strong>Quantitative</strong></h3>
<ul>
<li class="fragment">Handling Imbalanced Datasets</li>
<li class="fragment">Predictive Analytics and Student Performance</li>
<li class="fragment">Scaling and Automating Data Collection</li>
</ul>
</div>
</div>
</div>
<aside class="notes">
<p><strong>Qualitative</strong> - 1. Manual qualitative coding is time-consuming and often subject to coder bias or variability.</p>
<ul>
<li><ol start="2" type="1">
<li>achieving data saturation ensures the robustness and comprehensiveness of the qualitative analysis, affirming that the research findings are well-grounded in the data and that the developed themes are fully explored. However, it can be daunting, as it requires extensive data collection and interpretation until no new information emerges.</li>
</ol></li>
<li><ol start="3" type="1">
<li>Qualitative research that gathers data in multiple languages often faces hurdles in translation and maintaining the nuances of responses.</li>
</ol></li>
</ul>
<p><strong>Quantitative</strong> -</p>
<ol type="1">
<li>In educational research, certain populations or phenomena might be underrepresented in datasets, leading to biased predictions and analyses.</li>
</ol>
<ul>
<li><ol start="2" type="1">
<li>Predicting student performance and outcomes traditionally relies on static or limited datasets, which may not effectively capture the complexity of student learning trajectories.</li>
</ol></li>
<li><ol start="3" type="1">
<li>Collecting large-scale quantitative data efficiently while ensuring the quality and reliability of the data can be challenging, especially in diverse educational settings.</li>
</ol></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-can-help-with-these-challenges" class="slide level2" data-background-image="img/background.jpg">
<h2>What can help with these challenges</h2>

<img data-src="img/ai_santa.jpg" class="r-stretch"></section>
<section id="placement-of-llms" class="slide level2" data-background-image="img/background.jpg">
<h2>Placement of LLMs</h2>
<p><img data-src="img/ai.png" class="absolute fragment" style="top: 100px; left: 120px; width: 650px; "></p>
<p><img data-src="img/ml.png" class="absolute fragment" style="top: 100px; left: 120px; width: 650px; "></p>
<p><img data-src="img/NLP-keep.png" class="absolute fragment" style="top: 100px; left: 120px; width: 650px; "></p>
<p><img data-src="img/DL-keep.png" class="absolute fragment" style="top: 100px; left: 120px; width: 650px; "></p>
<p><img data-src="img/genai-keep.png" class="absolute fragment" style="top: 100px; left: 120px; width: 650px; "></p>
<p><img data-src="img/llm-keep.png" class="absolute fragment" style="top: 100px; left: 120px; width: 650px; "></p>
<aside class="notes">
<ul>
<li><p>Artificial intelligence or AI encompasses all technologies and methodologies that enable machines to mimic human intelligence and behavior.</p></li>
<li><p>Machine learning or ML is a subset of AI, focuses on algorithms that allow computers to learn from and make decisions based on data.</p></li>
<li><p>Natural Language Processing or NLP is a specific branch of AI and ML dedicated to giving computers the ability to understand, interpret, and respond to human language in a meaningful and useful way.</p></li>
<li><p>Deep Learning, nested within ML (and pertinent to NLP), utilizes neural networks with multiple layers to learn from vast amounts of data, often driving the core capabilities of modern NLP.</p></li>
<li><p>Generative AI, which often utilizes deep learning techniques, focuses on generating new content based on learned data patterns. It can be considered a part of NLP when the content generated is text.</p></li>
<li><p>LLMs or large language models has advanced AI tools designed to understand, generate, and interact with human language using vast amounts of text data. LLMs as a specific implementation within Generative AI (and by extension NLP), focus on generating and understanding human-like text at scale.</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="large-language-models" class="slide level2" data-background-image="img/background.jpg">
<h2>Large Language Models</h2>
<blockquote>
<p>Are <code>pre-trained</code> and then <code>fine-tuned</code> for purposes to solve common language problems (Wei et al, 2022b).</p>
</blockquote>
<p><img data-src="img/text_class.png" class="absolute fragment" style="top: 300px; left: 5px; width: 300px; "></p>
<p><img data-src="img/text_gen.png" class="absolute fragment" style="top: 300px; left: 325px; width: 300px; "></p>
<p><img data-src="img/text_summ.png" class="absolute fragment" style="top: 325px; left: 600px; width: 300px; "></p>
<p><img data-src="img/ques_ans.png" class="absolute fragment" style="top: 325px; left: 850px; width: 300px; "></p>
<aside class="notes">
<ul>
<li><p>LLMs are sophisticated models like GPT (from OpenAI) and BERT (from Google) that leverage NLP techniques to perform tasks like text generation, comprehension, translation, and more.</p></li>
<li><p>They do this by fine tuning for purposes to solve common language problems like:</p></li>
</ul>
<p><strong>Text Classification:</strong> Text classification involves assigning categories or labels to textual data based on its content. IE: Classify open-ended student responses into categories such as ‚Äúreflective,‚Äù ‚Äúdescriptive,‚Äù or ‚Äúanalytical‚Äù based on the content of their essays.</p>
<p><strong>Text Generation:</strong> Text generation is the process of automatically creating coherent text based on input prompts or data. IE: Using text generation to automatically create essay prompts or discussion questions that are tailored to the specific content of a course module.</p>
<p><strong>Document Summarization:</strong> Document summarization involves creating a concise and coherent summary of a longer document while retaining the key points and overall meaning. IE: This could be used by educational researchers to quickly summarize lengthy student essays or academic papers to streamline the grading and feedback process.</p>
<p><strong>Question Answering:</strong> Generate answers to questions posed by users, based on the information available in a given text or corpus. Researchers could develop a question answering system to provide students with instant feedback on inquiries about historical events or scientific concepts, drawing from a database of educational materials or textbooks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="prompt-engineering-method" class="slide level2" data-background-image="img/background.jpg">
<h2>Prompt Engineering (method)</h2>
<blockquote>
<p>Prompt Engineering is the skillful combination of art and science used to craft prompts that effectively draw specific responses from AI models (Akinwande et al., 2023, Augusto, 2023).</p>
</blockquote>
<p><img data-src="img/PE_elements.png">{width= 550px}</p>
<h4>
<p><a href="https://drive.google.com/file/d/1d9G7C3YIhluDhjfysglUJ-Ch5D5Tn2DV/view?usp=sharing" data-preview-link="auto">Prompt Exercise - click to try it üîó</a></p>
<aside class="notes">
<p>A prompt contains any of the following elements:</p>
<p>Instruction - a specific task or instruction you want the model to perform.</p>
<p>Context - external information or additional context that can steer the model to better responses.</p>
<p>Input Data - the input or question that we are interested to find a response for.</p>
<p>Output Indicator - the type or format of the output.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</h4></section>
<section id="techniques-of-prompt-engineering" class="slide level2" data-background-image="img/background.jpg">
<h2>Techniques of Prompt Engineering</h2>
<ul>
<li class="fragment">Few-Shot (Brown et al., 2023)</li>
<li class="fragment">In-context Learning (ICL) (Dong et al., 2023)</li>
<li class="fragment">Chain-of-Thought (COT) (Wei et al., 2022)</li>
<li class="fragment">Assertion Enhanced Few-Shot Learning (AEFL) (Shiariar et al., 2023).</li>
</ul>
<aside class="notes">
<p>Brown asserts that Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning.</p>
<p>Few-shot learning refers to the ability of a machine learning model to learn a new task from a very small amount of training data‚Äîspecifically, only a few examples (shots). The idea is to design models that can generalize from very limited data, somewhat like how humans can often learn new concepts with just a few examples. This method is contrasted with zero-shot learning, where no examples are provided, and many-shot learning, where many examples are used for training.</p>
<p>Incontext Learning - The model uses the context given in the prompt to infer the task requirements and generate appropriate responses.</p>
<p>Chain of thought Wei a language model is used in a way that mimics human problem-solving. When you use a language model with a chain of thought approach, you essentially guide the model through a sequence of logical steps to solve a problem.</p>
<p>Shiariar et al.&nbsp;introduced a method called Assertion Enhanced Few-Shot Learning (AEFL), which is a technique designed to improve the performance of large language models (LLMs) when they are given very few examples to learn from. This approach involves embedding domain-specific assertions into the prompts given to the models. Let me break this down:</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="two-case-studies" class="slide level2" data-background-image="img/background.jpg">
<h2>Two case studies</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div class="fragment">
<p><strong>Qualitative</strong></p>
<div class="r-fit-text">
<p><span class="story">STUDY 1</span> <a href="https://drive.google.com/file/d/1O99ZYAxYR_hHGDYvC7bVr-otIpvScuom/view?usp=sharing" data-preview-link="auto"><code>It's All About the Prompt: Deductive Coding's Role in AI vs. Human Performance.</code> üîó</a></p>
<p>Jeanne McClure, Daria Smyslova, Amanda Hall &amp; Shiyan Jiang</p>
<h3>
RQ:
</h3>
<p>How do large language models perform in deductive coding tasks in terms of accuracy, precision, and error patterns compared to human coders?</p>
</div>
</div>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:50%;">
<div class="fragment">
<p><strong>Quantitative</strong></p>
<div class="r-fit-text">
<p><span class="story">STUDY 2</span> <a href="https://drive.google.com/file/d/1RaL3NX38L1zOIowtypZBicSvDUf0aLRH/view?usp=sharing" data-preview-link="auto"><code>Leveraging Targeted Assertions in LLMs to Overcome Imbalances in Complex Educational Text Data.</code> üîó</a></p>
<p>Jeanne McClure, Machi Shimmei, Noboru Matsuda, Shiyan Jiang</p>
<p>RQ: How do enhancements like prompt engineering (PE) and the integration of assertions improve the performance of large language models compared to traditional machine learning algorithms in addressing the challenges of imbalanced textual educational datasets?</p>
</div>
</div>
</div>
</div>
</section>
<section>
<section id="study-1-exploring-the-efficacy-of-llms-in-qualitative-data-analysis" class="title-slide slide level1 center" data-background-color="#2596be">
<h1><span class="story">STUDY 1</span> <span style="float:right;text-align:right;">üí¨ Exploring the Efficacy of LLMs in Qualitative Data Analysis</span></h1>

</section>
<section class="slide level2">

<h3 id="study-1-context-exploring-the-efficacy-of-llms-in-qualitative-data-analysis"><span class="story">STUDY 1</span> üí¨ CONTEXT: Exploring the Efficacy of LLMs in Qualitative Data Analysis</h3>
<div class="columns">
<div class="column" style="width:40%;">
<div class="fragment">
<div class="r-fit-text">
<p><strong>METHODOLOGY:</strong></p>
<p><code>Prompt Techniques used</code></p>
<ul>
<li class="fragment">One-Shot/Few-Shot (Brown et al., 2020)</li>
<li class="fragment">In-context Learning (ICL) (Dong et al., 2023)</li>
<li class="fragment">Chain-of-Thought (COT) (Wei et al., 2022)</li>
</ul>
<p><code>Participants</code></p>
<ul>
<li class="fragment">Two human coders and Open AI LLMs</li>
</ul>
</div>
</div>
<div class="fragment">
<div class="r-fit-text">
<p><code>Experiments Conducted</code></p>
<ul>
<li class="fragment">‚Äú<a href="https://docs.google.com/document/d/1aKNw0rDXuPgLWN0eZ3FpgFCtufozZWixF6rS3L663Ug/edit?usp=sharing">Three experimentsüîó</a>‚Äù comparing the performance of human coders and LLMs.</li>
</ul>
</div>
</div>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:50%;">
<div class="fragment">
<div class="r-fit-text">
<p><code>Contextual Background:</code></p>
<ul>
<li class="fragment"><p>The study investigates Cognitive Engagement (CE) in an AI curriculum at a High School, analyzing student responses across three modules.</p></li>
<li class="fragment"><p>This mixed-method approach addresses both quantitative and qualitative aspects of coding, examining the depth and precision LLMs can offer in interpreting complex student responses.</p></li>
</ul>
<p><code>Significance:</code></p>
<ul>
<li class="fragment"><p>Demonstrates LLMs‚Äô potential in enhancing qualitative research methodologies.</p></li>
<li class="fragment"><p>Explores strategic prompt engineering to maximize LLM performance.</p></li>
</ul>
</div>
</div>
</div>
</div>
<aside class="notes">
<p>First Experiment: This initial phase featured a subset of data, and the task was structured with a streamlined Chain-of-Thought (COT) prompt. The experiment was designed to differentiate levels of cognitive engagement‚ÄîPassive, Active, and Constructive‚Äîusing simple categorization tasks.</p>
<p>Second Experiment: This phase reintroduced the COT prompt but augmented it with Few-Shot learning examples. The task structure was tripartite, involving a question, a response, and a label, providing a more nuanced evaluation across the same cognitive engagement categories.</p>
<p>Third Experiment: The final experiment retained elements from the previous two but incorporated a component from Assertion Enhanced Few-Shot Learning (AEFL) domain specific examples. This addition introduced a ‚ÄúReasoning‚Äù component to the task, aiming to enhance the depth and accuracy of the LLM‚Äôs coding by more closely mimicking human reasoning processes.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="findings-exploring-the-efficacy-of-llms-in-qualitative-data-analysis" class="slide level2">
<h2>üí¨ FINDINGS: Exploring the Efficacy of LLMs in Qualitative Data Analysis</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div class="fragment">
<div class="r-fit-text">
<p><strong>FINDINGS:</strong></p>
<div class="box">
<ul>
<li class="fragment">Zero shot and COT LLM outperfomed human coders</li>
<li class="fragment">COT with Few Shot improved LLM performance in minority class</li>
<li class="fragment">COT, Few Shot with domain specific reasoning improved LLM and Human coders</li>
</ul>
</div>
</div>
</div>
</div><div class="column" style="width:60%;">
<div class="fragment">
<div class="r-fit-text">
<p><img data-src="img/accuracy_score.png" class="absolute fragment" style="top: 200px; left: 450px; width: 700px; "></p>
</div>
</div>
</div>
</div>
<aside class="notes">
<p>LLMs consistently demonstrated superior precision and accuracy across various engagement categories,</p>
<ul>
<li><p>Experiment 1: LLMs showed higher precision than human coders, particularly effective in Passive and Active engagement categorization. LLMs utilize built-in knowledge and reasoning capabilities to make deductions, thus demonstrating the importance of the inherent design and capabilities of the model‚Äôs architecture in prompt engineering. The results highlighted the foundational effectiveness of CoT prompting, setting a baseline for how LLMs can handle complex cognitive tasks in a raw format without task-specific tuning.</p></li>
<li><p>Experiment 2: LLMs demonstrated significant improvements in accuracy, particularly excelling in Active engagement coding with enhanced precision. Enhanced the performance of LLMs significantly, showing better accuracy and handling of more complex coding tasks. Few-shot learning allowed the LLMs to leverage specific examples to better understand and classify the data, illustrating the effectiveness of providing contextual learning aids. The CoT component helped in structuring the reasoning process more clearly, which improved the LLM‚Äôs ability to follow and apply logical steps in deductive coding</p></li>
<li><p>Experiment 3: This experiment showed the highest level of performance among the three, with LLMs effectively using the combined strategies of AEFL and structured CoT to maximize precision and minimize errors. The AEFL technique, which incorporates domain-specific assertions, further refined the model‚Äôs understanding and response accuracy, especially in more nuanced or ambiguous scenarios. This sophisticated prompt engineering approach demonstrated that well-designed prompts significantly boost LLM effectiveness, particularly in handling complex, imbalanced datasets typical in educational contexts.</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="study-2-overcoming-data-imbalances-with-llms-in-educational-research" class="title-slide slide level1 center" data-background-color="#2596be">
<h1><span class="story">STUDY 2</span> <span style="float:right;text-align:right;">üîë Overcoming Data Imbalances with LLMs in Educational Research</span></h1>

</section>
<section class="slide level2">

<h3 data-background-image="img/background.jpg" id="study-2-overcoming-data-imbalances-with-llms"><span class="story">STUDY 2</span> üîë Overcoming Data Imbalances with LLMs</h3>
<div class="columns">
<div class="column" style="width:40%;">
<div class="fragment">
<div class="r-fit-text">
<p><strong>METHODOLOGY:</strong></p>
<p><code>Prompt Techniques used</code></p>
<ul>
<li class="fragment">In-context Learning (ICL)</li>
<li class="fragment">Chain-of-Thought (COT) (Wei et al., 2022)</li>
<li class="fragment">Assertion Enhanced Few-Shot Learning (AEFL) (Shiariar et al., 2023)</li>
</ul>
<p><code>Data and Participants</code> - Analyzed responses from 28 students across varied demographics - Evaluated on the Training dataset from original study (<em>N</em>=135)</p>
</div>
</div>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:50%;">
<div class="fragment">
<div class="r-fit-text">
<p><code>Context</code></p>
<ul>
<li class="fragment">Evaluate LLMS ability to automatically classify Cognitive Engagement from adapted CAP framework (Constructive - Active - Passive modes) compared to traditional Machine learning.</li>
</ul>
<p><img data-src="img/assertion.png" class="absolute fragment" style="top: 300px; left: 550px; width: 550px; "></p>
</div>
</div>
</div>
</div>
</section>
<section id="overcoming-data-imbalances-with-llms" class="slide level2" data-background-image="img/background.jpg">
<h2>üîë Overcoming Data Imbalances with LLMs</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div class="fragment">
<div class="r-fit-text">
<p><strong>FINDINGS:</strong></p>
<div class="box">
<ul>
<li class="fragment">Performance: LLMs excelled in minority classes, notably with a Constructive class F1 score of 32.</li>
<li class="fragment">Comparison: Outperformed traditional ML models, particularly in handling data imbalances.</li>
<li class="fragment">Assertions: Improved accuracy and reduced misclassifications by effectively addressing contextual complexities.</li>
</ul>
</div>
</div>
</div>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:50%;">
<p><img data-src="img/tble_compare.png" class="absolute fragment" style="top: 50px; left: 500px; width: 575px; "></p>
<p><img data-src="img/precision2.png" class="absolute fragment" style="top: 375px; left: 500px; width: 575px; "></p>
</div>
</div>
<aside class="notes">
<p>Experiment 1: This basic prompting helped establish a baseline understanding of how LLMs interpret and classify cognitive engagement from textual data, highlighting the initial benefits of structured reasoning in enhancing model accuracy.</p>
<p>Experiment 2: The combination of Few-Shot Learning with Enhanced CoT Prompting significantly improved the LLM‚Äôs ability to discern between different levels of cognitive engagement, reducing misclassification and demonstrating the impact of contextualized learning on model performance.</p>
<p>Experiment 3: The integration of assertions allowed the LLM to handle ambiguities and nuances more effectively, showcasing a substantial improvement in precision and the ability to correctly interpret complex student responses.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="llms-in-education-ethical-challenges-and-future-directions" class="slide level2" data-background-image="img/background.jpg">
<h2>LLMs in Education: Ethical Challenges and Future Directions</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="fragment">
<h3>
Ethical challenges:
</h3>
<ul>
<li class="fragment"><p><strong>Hallucinations</strong> (Wu et al., 2022).</p></li>
<li class="fragment"><p><strong>Inconsistency</strong> (Hase et al., 2021).</p></li>
<li class="fragment"><p><strong>Specialization Needs</strong></p></li>
</ul>
<h3>
Future Directions:
</h3>
<ul>
<li class="fragment"><p><strong>Refining Prompt Design</strong></p></li>
<li class="fragment"><p><strong>Real-time Tools</strong></p></li>
<li class="fragment"><p><strong>Impact Assessments</strong></p></li>
</ul>
</div>
</div><div class="column" style="width:50%;">
<div class="fragment">
<h3>
Transformative Potential:
</h3>
<ul>
<li class="fragment"><p><strong>Empowering Educators</strong></p></li>
<li class="fragment"><p><strong>Innovative Pedagogy</strong></p></li>
<li class="fragment"><p><strong>Data Management</strong></p></li>
</ul>
</div>
</div>
</div>
<aside class="notes">
<p>Hallucinations: LLMs may generate plausible but incorrect information. Inconsistency: Variability in output from similar prompts affects reliability. Specialization Needs: Accuracy requires tailored approaches for different educational fields.</p>
<ul>
<li><p><strong>Refining Prompt Design:</strong> Enhancing precision and adapting to multilingual contexts to increase relevance and applicability.</p></li>
<li><p><strong>Real-time Tools:</strong> Developing interactive tools that offer personalized learning experiences.</p></li>
<li><p><strong>Impact Assessments:</strong> Conducting comprehensive studies to evaluate the long-term educational outcomes and address ethical considerations.</p></li>
</ul>
<p>Empowering Educators: Reducing routine tasks to focus on pedagogy. Innovative Pedagogy and Inclusive Education: Promoting dynamic teaching methods and ensuring fairness. Data Management: Improving handling of imbalanced datasets in education.</p>
<p><strong>Conclusion:</strong> The integration of LLMs into educational practices is not just about enhancing existing methods but transforming the educational landscape to foster a more interactive, personalized, and efficient learning environment. As we continue to refine these technologies and their applications, the potential for significant, positive change in education systems worldwide becomes increasingly tangible.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="acknowledgements" class="slide level2" data-background-image="img/background.jpg">
<h2>Acknowledgements</h2>
<div class="r-fit-text">
<blockquote>
<p>Thank you to my Advisor and Chair, <em>Shiyan Jiang</em>. Additionally, thank you to <em>Noboru Matsuda‚Äôs</em> lab in Computer Science department and Tasmia Shairiar. A special thanks to Machi Simmei, Daria Smyslova and Amanda Hall for their collaborations on our papers.</p>
</blockquote>
<p><br><br></p>
<blockquote>
<p>This material is based upon work supported by the National Science Foundation under Grant No.&nbsp;DRL-1949110. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
</blockquote>
</div>
</section>
<section id="references" class="slide level2" data-background-image="img/background.jpg">
<h2>References</h2>
<div class="r-fit-text">
<h4>
<p>Akinwande, V., Jiang, Y., Sam, D., &amp; Kolter, J. Z. (2023). Understanding prompt engineering may not require rethinking generalization. arXiv preprint arXiv:2310.03957.</p>
<p>Augusto C., P. (2023, April 20). The importance of prompt engineering in natural language systems. LinkedIn. https://www.linkedin.com/pulse/importance-prompt-engineering-natural-language-c-cardoso-r-/</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., et al.&nbsp;(2020). Language models are few-shot learners. In Advances in Neural Information Processing Systems 33 (pp.&nbsp;1877‚Äì1901).</p>
<p>Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., ‚Ä¶ &amp; Sui, Z. (2022). A survey on in-context learning. arXiv preprint arXiv:2301.00234.</p>
<p>Hase, P., Diab, M., Celikyilmaz, A., Li, X., Kozareva, Z., Stoyanov, V., ‚Ä¶ &amp; Iyer, S. (2021). Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs. arXiv preprint arXiv:2111.13654.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., ‚Ä¶ &amp; Zhou, D. (2022a). Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35, 24824-24837.</p>
<p>Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., ‚Ä¶ &amp; Fedus, W. (2022b). Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.</p>
<p>Wu, T., Terry, M., &amp; Cai, C. J. (2022, April). Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. In Proceedings of the 2022 CHI conference on human factors in computing systems (pp.&nbsp;1-22).</p>
<p>Shahriar, T., Matsuda, N., &amp; Ramos, K. (2023). Assertion Enhanced Few-Shot Learning: Instructive Technique for Large Language Models to Generate Educational Explanations. arXiv preprint arXiv:2312.03122.</p>
</h4>
</div>
<div class="footer footer-default">
<p><a href="https://www.go.ncsu.edu/llms-cafe">go.ncsu.edu/llms-cafe</a></p>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="Index_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="Index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="Index_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="Index_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="Index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="Index_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="Index_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="Index_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="Index_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="Index_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>